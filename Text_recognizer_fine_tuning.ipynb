{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manvento/Debriefing-automatico-grazie-alla-Computer-Vision-/blob/main/Text_recognizer_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21ceb49e-40bc-4ba9-b9df-9d43ed47601e",
      "metadata": {
        "id": "21ceb49e-40bc-4ba9-b9df-9d43ed47601e",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Text recognizer fine tuning\n",
        "\n",
        "Questo notebook rappresenta gli experiments realizzati inizialmente per cercare una soluzione al riconoscimento dei display digitali.\n",
        "Non è il modello definitivo, ma viene riportato per dare un esempio delle attività svolte."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fase iniziale per Google Colab\n",
        "\n",
        "Questa sezione è necessaria soltanto se si usa Google Colab."
      ],
      "metadata": {
        "id": "DxyskMtROokr",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "id": "DxyskMtROokr"
    },
    {
      "cell_type": "code",
      "source": [
        "# Here you must insert your Google Drive id to the data file, that is a compressed file containing the following entries.\n",
        "# - pretrained_model (if you have any)\n",
        "# - test\n",
        "# - train \n",
        "# - valid\n",
        "# last three folders must contain the annotated dataset for test, training and validation\n",
        "\n",
        "data_id = '1KVJE_fv601htgE5l_2oCTjHaieMaCihm'"
      ],
      "metadata": {
        "id": "3FahK5RaQOOx",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "id": "3FahK5RaQOOx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "  print(\"You are using Google CoLab, so we need to donwload data compressed folder from Google Drive\")\n",
        "  !pip install -U -q PyDrive\n",
        "  import os\n",
        "  from pydrive.auth import GoogleAuth\n",
        "  from pydrive.drive import GoogleDrive\n",
        "  from google.colab import auth\n",
        "  from oauth2client.client import GoogleCredentials\n",
        "\n",
        "  # 1. Authenticate and create the PyDrive client.\n",
        "  auth.authenticate_user()\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  drive = GoogleDrive(gauth)\n",
        "\n",
        "  # choose a local (colab) directory to store the data.\n",
        "  local_download_path = os.path.expanduser('/content')\n",
        "  try:\n",
        "    os.makedirs(local_download_path)\n",
        "  except: pass\n",
        "\n",
        "  # download compressed file content\n",
        "  f = drive.CreateFile({'id': data_id})\n",
        "  fname = os.path.join(local_download_path, f['title'])\n",
        "  f.GetContentFile(fname)\n",
        "\n",
        "  import zipfile\n",
        "  with zipfile.ZipFile(f['title'], 'r') as zip_ref:\n",
        "      zip_ref.extractall('.')\n"
      ],
      "metadata": {
        "id": "4FdZpF54DlH0",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "id": "4FdZpF54DlH0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4d156ed1-5c2d-448e-a4d1-3ac8f762b6f3",
      "metadata": {
        "id": "4d156ed1-5c2d-448e-a4d1-3ac8f762b6f3",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Installazione librerie aggiuntive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e001340",
      "metadata": {
        "scrolled": true,
        "id": "0e001340",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "if not 'google.colab' in str(get_ipython()):\n",
        "  !pip install tensorflow~=2.6.0\n",
        "  !pip install --upgrade matplotlib\n",
        "\n",
        "!pip install keras_ocr\n",
        "!pip install Pillow\n",
        "!pip install opencv-python-headless==4.5.1.48"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ff5836d",
      "metadata": {
        "id": "8ff5836d",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Importing the needed libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0c0640e",
      "metadata": {
        "id": "e0c0640e",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import math\n",
        "import string\n",
        "import shutil\n",
        "import keras_ocr\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5e3674d-874f-46bb-9201-f9cf41324895",
      "metadata": {
        "tags": [],
        "id": "b5e3674d-874f-46bb-9201-f9cf41324895",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Caratteristiche dei display digitali:\n",
        "\n",
        "- font specifici per il digitale\n",
        "- basso contrasto (grigio su grigio o nero su verde scuro)\n",
        "- presenza di rumore nell'immagine\n",
        "- bassa risoluzione"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb5b5218-5d91-471d-952b-01603fe47934",
      "metadata": {
        "id": "cb5b5218-5d91-471d-952b-01603fe47934",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "display(Image.open('test/images/01.jpg'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cddaee3-848b-42ba-9ff7-d3fdd2c8d334",
      "metadata": {
        "id": "0cddaee3-848b-42ba-9ff7-d3fdd2c8d334",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Definizione di funzioni di utilità\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "918d73cb-f8b9-4785-9aa3-3aff2d167419",
      "metadata": {
        "id": "918d73cb-f8b9-4785-9aa3-3aff2d167419",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# preprocessing and postprocessing\n",
        "\n",
        "def fit(image):\n",
        "    fitted = None\n",
        "    width = 200\n",
        "    height = 31\n",
        "    x_scale = width / image.shape[1]\n",
        "    y_scale = height / image.shape[0]\n",
        "    if x_scale == 1 and y_scale == 1:\n",
        "        fitted = image\n",
        "        scale = 1\n",
        "    elif x_scale <= y_scale:\n",
        "        scale = width / image.shape[1]\n",
        "        resize_width = width\n",
        "        resize_height = (width / image.shape[1]) * image.shape[0]\n",
        "    else:\n",
        "        scale = height / image.shape[0]\n",
        "        resize_height = height\n",
        "        resize_width = scale * image.shape[1]\n",
        "\n",
        "    if fitted is None:\n",
        "        resize_width, resize_height = map(int, [resize_width, resize_height])\n",
        "        fitted = np.zeros((height, width, 3), dtype=\"uint8\")\n",
        "\n",
        "        # opencv resize function raise an error if applied to a (1, 1, 3) image\n",
        "        if image.shape[0] != 1 or image.shape[1] != 1:\n",
        "            image = cv2.resize(image, dsize=(resize_width, resize_height))\n",
        "            fitted[: image.shape[0], : image.shape[1]] = image[:height, :width]\n",
        "    return fitted\n",
        "\n",
        "def preprocess(img):\n",
        "    img = fit(img)\n",
        "    img = cv2.cvtColor(img, code=cv2.COLOR_RGB2GRAY)[..., np.newaxis] if img.shape[-1] == 3 else img\n",
        "    img = img / 255\n",
        "    img = img[np.newaxis, ...]\n",
        "    img = np.asarray(img, dtype=np.float32)\n",
        "    return img\n",
        "\n",
        "def output_to_sentence(output, alphabet):\n",
        "    return \"\".join([alphabet[i] if i != -1 else \"\" for i in output[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c92a1ead-680b-45a1-a19f-6295c79c7d10",
      "metadata": {
        "id": "c92a1ead-680b-45a1-a19f-6295c79c7d10",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# helper function used to show the result on test images\n",
        "\n",
        "def test_on_images(images, preproc_images, recognizer):\n",
        "    num_cols = 4\n",
        "    num_rows = math.ceil(len(images) / num_cols)\n",
        "\n",
        "    x_dim = 25\n",
        "    y_dim = math.ceil(num_rows / num_cols * 10)\n",
        "\n",
        "    fig, axs = plt.subplots(num_rows, num_cols)\n",
        "    fig.set_size_inches(x_dim, y_dim)\n",
        "\n",
        "\n",
        "    for i in range(num_cols * num_rows):\n",
        "        x = i // num_cols\n",
        "        y = i % num_cols\n",
        "        axs[x, y].set_xticks([])\n",
        "        axs[x, y].set_yticks([])\n",
        "\n",
        "        if i >= len(images):\n",
        "            continue\n",
        "\n",
        "        output = recognizer.prediction_model.predict(preproc_images[i])\n",
        "        text_read = output_to_sentence(output, recognizer.alphabet)\n",
        "\n",
        "        img = cv2.cvtColor(images[i], cv2.COLOR_BGR2RGB)\n",
        "        axs[x, y].imshow(img)\n",
        "        axs[x, y].set_title(f'{paths[i]} - {text_read}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5096f2a-74f7-4423-9092-066bde60482d",
      "metadata": {
        "id": "a5096f2a-74f7-4423-9092-066bde60482d",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Come si comporta il modello keras generico?\n",
        "\n",
        "La prima fase è sempre quella di chiedersi: posso usare qualcosa di già pronto? Bene, allora vediamo come funziona il modello keras senza fitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e9c4393-0be6-463a-b3c4-462c4ccd2407",
      "metadata": {
        "id": "0e9c4393-0be6-463a-b3c4-462c4ccd2407",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "recognizer = keras_ocr.recognition.Recognizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ad6e5fa-8af6-4860-aa47-8f883d18461d",
      "metadata": {
        "tags": [],
        "id": "4ad6e5fa-8af6-4860-aa47-8f883d18461d",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "paths = sorted(glob.glob(os.path.join('test', 'images', '*'))[:20])\n",
        "images = [cv2.imread(path) for path in paths]\n",
        "preproc_images = [preprocess(img) for img in images]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73a26933-4a33-4523-925d-0ac3386c916f",
      "metadata": {
        "id": "73a26933-4a33-4523-925d-0ac3386c916f",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "test_on_images(images, preproc_images, recognizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c565c593-d572-47f4-84a8-6bea5b5769d2",
      "metadata": {
        "id": "c565c593-d572-47f4-84a8-6bea5b5769d2",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Come notiamo, il modello keras-ocr standard ottiene pessimi risultati nella lettura di font digitali, mentre funziona molto meglio in testi normali (il caso in alto a destra).\n",
        "\n",
        "Qua non l'abbiamo mostrato, ma un altro problema con un modello senza fitting è dovuto al contrasto: tende a preferire i testi con contrasto alto dando meno importanza, ad esempio, ai display digitali. E' per questo che abbiamo realizzato un primo modello che esclude tutto ciò che sta fuori dal display stesso."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2f9c434-db3f-4cfb-bd77-4f4db3e17c12",
      "metadata": {
        "id": "b2f9c434-db3f-4cfb-bd77-4f4db3e17c12",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Fase di addestramento"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80ff7851",
      "metadata": {
        "id": "80ff7851",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## parametri dell'algoritmo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f5e085c",
      "metadata": {
        "id": "2f5e085c",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "patience = 10\n",
        "training_epochs = 1000\n",
        "\n",
        "model_file_name = 'trained_model'\n",
        "\n",
        "print(f'Parameters:\\n' \\\n",
        "      f'\\nbatch_size: {batch_size}' \\\n",
        "      f'\\ntraining_epochs: {training_epochs}' \\\n",
        "      f'\\npatience: {patience}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "950894a7",
      "metadata": {
        "tags": [],
        "id": "950894a7",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Definizione dell'alfabeto\n",
        "\n",
        "Usiamo un generatore sintetico di immagini e gli indichiamo quali sono i testi ammissibili per il nostro caso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86f38492",
      "metadata": {
        "id": "86f38492",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# the alphabet is expanded with characters that are present in the displays and in the generated/tagged images\n",
        "\n",
        "SPECIAL_CHARS = [' ', '+', '-', '.', ':', '=']\n",
        "\n",
        "alphabet = string.digits + string.ascii_letters + ''.join(SPECIAL_CHARS)\n",
        "custom_recognizer_alphabet = ''.join(sorted(set(alphabet.lower())))\n",
        "print(f\"The alphabet to recognize is: {custom_recognizer_alphabet}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ef5bf4f",
      "metadata": {
        "id": "3ef5bf4f",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "print(\"Getting and compiling the recognizer...\")\n",
        "\n",
        "custom_recognizer = keras_ocr.recognition.Recognizer(\n",
        "    alphabet=custom_recognizer_alphabet\n",
        ")\n",
        "custom_recognizer.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ad2df7f-7ff7-4baa-ba7c-a0ff9fd39469",
      "metadata": {
        "id": "0ad2df7f-7ff7-4baa-ba7c-a0ff9fd39469",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "_Come indicato dall'output, visto che l'alfabeto è diverso da quello del training originale, mantiene i pesi soltanto per i primi layer, quelli di feature extraction._"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a88da23",
      "metadata": {
        "id": "6a88da23",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Creiamo due liste di tuple contententi le immagini per training e validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2aaff659-cb6c-4126-9a5d-49c082e25d07",
      "metadata": {
        "id": "2aaff659-cb6c-4126-9a5d-49c082e25d07",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "img_labels = {}\n",
        "\n",
        "for mode in ['train', 'valid']:\n",
        "    with open(os.path.join(mode, 'pairs.txt')) as f:\n",
        "        lines = f.read().splitlines()\n",
        "\n",
        "    img_labels[mode] = [tuple(line.split('\\t')) for line in lines]\n",
        "    img_labels[mode] = [(img, None, label) for img, label in img_labels[mode]]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74b5deb1-1f27-4bcd-bbcf-55cd3292affa",
      "metadata": {
        "id": "74b5deb1-1f27-4bcd-bbcf-55cd3292affa",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "#### Vediamo, per esempio, come sono fatte queste tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47c5664e-4b8e-4351-9d81-0531fd59f4b4",
      "metadata": {
        "id": "47c5664e-4b8e-4351-9d81-0531fd59f4b4",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "[print(line) for line in lines[:3]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9e9a803-9cdd-4f99-8987-71e97eef605a",
      "metadata": {
        "id": "e9e9a803-9cdd-4f99-8987-71e97eef605a",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "display(Image.open(lines[0].split('\\t')[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec708721-11b0-4da8-ad4b-d41b30f9a475",
      "metadata": {
        "id": "ec708721-11b0-4da8-ad4b-d41b30f9a475",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Ora creiamo degli image generator (che sono gli oggetti usati da Keras-OCR per leggere i dataset) in cui passiamo un le immagini che abbiamo generato oltre a quelle originali"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9f58c45",
      "metadata": {
        "id": "e9f58c45",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "print('Building training and validation \"image generators\" from the datasets...')\n",
        "(training_image_gen, training_steps), (validation_image_gen, validation_steps) = [\n",
        "    (\n",
        "        keras_ocr.datasets.get_recognizer_image_generator(\n",
        "            labels=labels,\n",
        "            height=custom_recognizer.model.input_shape[1],\n",
        "            width=custom_recognizer.model.input_shape[2],\n",
        "            alphabet=custom_recognizer_alphabet\n",
        "        ),\n",
        "        len(labels) // batch_size\n",
        "    ) for labels in [img_labels['train'], img_labels['valid']]\n",
        "]\n",
        "\n",
        "training_gen, validation_gen = [\n",
        "    custom_recognizer.get_batch_generator(\n",
        "        image_generator=image_generator,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "    for image_generator in [training_image_gen, validation_image_gen]\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd4ad21f-2ec3-4ac5-bb31-c3b79a261979",
      "metadata": {
        "id": "fd4ad21f-2ec3-4ac5-bb31-c3b79a261979",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "#### Ora definiamo delle callback utili per generare log da visualizzare in fase di training e il check sulla patience, per l'early stopping, ovvero terminiamo il training quando non ci sono miglioramenti nella validation loss per _n_ cicli, dove _n_ è proprio la patience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8279eba7",
      "metadata": {
        "id": "8279eba7",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "if os.path.isdir(model_file_name):\n",
        "    shutil.rmtree(model_file_name)\n",
        "\n",
        "os.mkdir(model_file_name)\n",
        "chk_file = os.path.join('.', model_file_name)\n",
        "log_file = os.path.join('.', model_file_name, 'epochs.csv')\n",
        "\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=patience, restore_best_weights=False),\n",
        "    tf.keras.callbacks.ModelCheckpoint(chk_file, monitor='val_loss', save_best_only=True),\n",
        "    tf.keras.callbacks.CSVLogger(log_file)\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d636b84-77c7-4c8e-b89d-cda0eac51679",
      "metadata": {
        "id": "3d636b84-77c7-4c8e-b89d-cda0eac51679",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "#### A questo punto si può procedere con il training vero e proprio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15e77014",
      "metadata": {
        "id": "15e77014",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "print('Training the custom recognizer...')\n",
        "\n",
        "# custom_recognizer.training_model.fit(\n",
        "#     training_gen,\n",
        "#     steps_per_epoch=training_steps,\n",
        "#     validation_steps=validation_steps,\n",
        "#     validation_data=validation_gen,\n",
        "#     callbacks=callbacks,\n",
        "#     epochs=training_epochs\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f384f9a9-1ab3-484b-9718-4a93480cedde",
      "metadata": {
        "id": "f384f9a9-1ab3-484b-9718-4a93480cedde",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Model testing\n",
        "\n",
        "Ora carichiamo i pesi del modello appena addestrato con l'alfabeto definito, e verifichiamo se siamo riusciti ad ottenere miglioramenti rispetto a Keras-OCR senza fine tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d905365-798d-4484-b58d-7dcb4b8a6477",
      "metadata": {
        "id": "5d905365-798d-4484-b58d-7dcb4b8a6477",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "pretrained_recognizer = keras_ocr.recognition.Recognizer(\n",
        "        alphabet=custom_recognizer_alphabet\n",
        "    )\n",
        "pretrained_recognizer.compile()\n",
        "pretrained_recognizer.model.load_weights('pretrained_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd658ba4-389a-4d46-8251-afc4dcfbe540",
      "metadata": {
        "id": "dd658ba4-389a-4d46-8251-afc4dcfbe540",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "test_on_images(images, preproc_images, pretrained_recognizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zo5mpps1UVnI",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "id": "Zo5mpps1UVnI",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "default:Python",
      "language": "python",
      "name": "conda-env-default-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}